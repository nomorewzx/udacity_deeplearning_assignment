{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(input_data, previous_output_data, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(input_data, ix) + tf.matmul(previous_output_data, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(input_data, fx) + tf.matmul(previous_output_data, fm) + fb)\n",
    "    new_candidate = tf.tanh(tf.matmul(input_data, cx) + tf.matmul(previous_output_data, cm) + cb)\n",
    "    update = input_gate * new_candidate\n",
    "    state = forget_gate * state + update\n",
    "    output_gate = tf.sigmoid(tf.matmul(input_data, ox) + tf.matmul(previous_output_data, om) + ob)\n",
    "    output = output_gate * tf.tanh(state)\n",
    "    return output, state\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for input_i in train_inputs:\n",
    "    output, state = lstm_cell(input_i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits = logits, labels = tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296488 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.02\n",
      "================================================================================\n",
      "oxavbr jagdb oeelygthesnjf keigoeld eo ueeage  frms bstj cydfxs orlieroh hxehrsa\n",
      "bnedeeei ekafp xlbtqsy fraec wcppbxbhn qvil tz kwa ppvthagfentwczcoylhichae lmc \n",
      "vnvn wiekvulum ke    fuesi eqoesaxfsbximu slebh a  lcojwtruhsajmanih kr ccaqq iq\n",
      "zsrprlvj gmkgvtkgpkesrfwukh it jres rj ib qnvple tiruopl  anfn mnl x ypolkopiar \n",
      "yxwtionkjkdgdnot  iijdzovjaor ipatagtoyyynvmjvwealuba gi mudi ytntepgo eiugqaigl\n",
      "================================================================================\n",
      "Validation set perplexity: 20.22\n",
      "Average loss at step 100: 2.588941 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.24\n",
      "Validation set perplexity: 10.66\n",
      "Average loss at step 200: 2.245368 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.51\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 300: 2.092827 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 8.14\n",
      "Average loss at step 400: 1.995264 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.42\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 500: 1.931877 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 600: 1.906294 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 700: 1.856329 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 800: 1.814667 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 900: 1.827440 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.06\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1000: 1.820307 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "================================================================================\n",
      "umer usee clas filliary impouting be sequeis of be trund the nan now hentber eve\n",
      "n signivily uning lecume of eight awove disending is a ecteru iv when be assood \n",
      "bel juria fleemer and sexale consorme bemo doed be teates af appany one five rel\n",
      "quanle asere unike colfertion as tored incevering sbiss with of bacay and part o\n",
      "qual used the norrea huthophe himsounit are the e as will of tepprivia leada bea\n",
      "================================================================================\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 1100: 1.773320 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 1200: 1.751662 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1300: 1.731249 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1400: 1.744090 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1500: 1.734827 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1600: 1.747231 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1700: 1.711432 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 1800: 1.667460 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 1900: 1.645512 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2000: 1.696209 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "================================================================================\n",
      "phistuaps of to by three as audiam warn hean how more lockest was mangensictra s\n",
      "dens on appasiage indies ralg of welloves usablys of four shevode seryeph to wit\n",
      "quathie argamings hrega in two edding conciplan subcon inciders vysinaps and and\n",
      "y reguirs wenes hevorn systepe of coppeed a suble musital apericany landers essl\n",
      "k itsionia beginary polico on seven rates worys who wott wife canith langing yom\n",
      "================================================================================\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2100: 1.686404 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2200: 1.681178 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2300: 1.638082 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 2400: 1.662084 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2500: 1.680694 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2600: 1.651615 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2700: 1.653060 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2800: 1.649351 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 2900: 1.647093 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3000: 1.652674 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "================================================================================\n",
      "k altedued budder addicemsted meadurach africald institure deque to foucs contis\n",
      "x particelsl from krimes betweer and due sumperien the eight moniom of populatio\n",
      "ration shatt recalt begind apup full and notemott langaddan the many and ause th\n",
      "f paus buills whichd r one nine nine nine four reanom bulle gacely beitherse dek\n",
      "hely systempt b one nine seven one one two one nine a seater of mothine at perie\n",
      "================================================================================\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3100: 1.631029 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3200: 1.647291 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3300: 1.638349 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3400: 1.667210 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3500: 1.658643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3600: 1.671454 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3700: 1.643873 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3800: 1.643945 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3900: 1.639600 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4000: 1.653462 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "an she edig the loged the see linging jumgensatulands mices termmanols aproutste\n",
      "roy are dir the verters vasa ring or projects weemonking of fortas instite or ha\n",
      "ch axverations be was one nine five one the enlinds and perious ho mone kou d va\n",
      "y fir one nine eilw classes doe actors devalisted  uthiester they srepters trout\n",
      "nams its arneraticy and offign to imperoramer three shew hole wams lite feection\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4100: 1.634394 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4200: 1.636519 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4300: 1.618506 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4400: 1.611362 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4500: 1.619676 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4600: 1.611349 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4700: 1.626473 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4800: 1.636723 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4900: 1.636477 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5000: 1.607349 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "================================================================================\n",
      "x hanolla cestly film incomenum was only kings equative obrove sur ded a pany wh\n",
      "gensal for pospmment rospolity as the self other or mountix enisyard s lay incep\n",
      "n is arn secred heamian acciuijus on two seven zero zero zero zero zero zero two\n",
      "ne the complyphreury poss one new one nine five one four nine six eight five ker\n",
      "omt the infiret five nightial conespirmud in the is was of who natually gondors \n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5100: 1.608606 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5200: 1.587929 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5300: 1.579326 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5400: 1.579877 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5500: 1.564485 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5600: 1.581146 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5700: 1.565164 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5800: 1.576319 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5900: 1.571297 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6000: 1.545839 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "y when grime not names as fown male partifind holis dim to resid work and the co\n",
      "is subable bong estalod defants and bolicius enistem of foreing when and particu\n",
      "beldew people lowes batkkakinal been war one nine five four three three eight se\n",
      "ver of one nine five eight floct time beegeer man in these by settorse be pawtwo\n",
      "rieinki his ungulvia the were was comem is codphnical about which one nine one t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6100: 1.562199 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6200: 1.534794 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6300: 1.541525 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6400: 1.543094 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6500: 1.559082 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6600: 1.596514 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6700: 1.584106 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6800: 1.605920 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6900: 1.580179 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 7000: 1.577926 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "quaries in they newlinke then it sa on basin lague in of ittever thiopmaille hem\n",
      "onary writer these araliv discoveriew usinstuation has assasks to park they in o\n",
      "hispessey powerth in german the opponeuge to wointwenery her diettental institur\n",
      "jacrarget all and one nine zero melani apoods carbiti capose of provides and the\n",
      "vern reject spotiborieacalip profextant in libere of the relumenting tume of ear\n",
      "================================================================================\n",
      "Validation set perplexity: 4.37\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "num_unrollings = 4\n",
    "learning_rate = 1\n",
    "batch_size = 64\n",
    "\n",
    "graph_one_matmul = tf.Graph()\n",
    "with graph_one_matmul.as_default():\n",
    "    weights_for_input = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "    weights_for_previous_output = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    biases = tf.Variable(tf.zeros([num_nodes*4]))\n",
    "    \n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    classifier_layer_weights = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    classifier_layer_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    def lstm_cell(input_data, previous_output, state):\n",
    "        gates_matmul = tf.matmul(input_data, weights_for_input) + tf.matmul(previous_output, weights_for_previous_output) + biases\n",
    "        \n",
    "        input_gate = tf.sigmoid(gates_matmul[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(gates_matmul[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        new_candidate = tf.tanh(gates_matmul[:, 2 * num_nodes : 3 * num_nodes])\n",
    "        output_gate = tf.sigmoid(gates_matmul[:, 3 * num_nodes :])\n",
    "        \n",
    "        update = input_gate * new_candidate\n",
    "        \n",
    "        state = forget_gate * state + update\n",
    "        \n",
    "        output = output_gate * tf.tanh(state)\n",
    "        \n",
    "        return state, output\n",
    "    \n",
    "    def unroll_lstm_cells(train_input_list, train_label_list):\n",
    "        output_list = list()\n",
    "        output = saved_output\n",
    "        state = saved_state\n",
    "        \n",
    "        for input_i in train_input_list:\n",
    "            output, state = lstm_cell(input_i, output, state)\n",
    "            output_list.append(output)\n",
    "        \n",
    "        return output, state, output_list\n",
    "\n",
    "    def clip_gradients_by_global_norm(optimizer):\n",
    "        gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        gradients_variables_tuple_list = zip(gradients, variables)\n",
    "        return gradients_variables_tuple_list\n",
    "    \n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    \n",
    "    train_input_list = train_data[:num_unrollings]\n",
    "    train_label_list = train_data[1:]\n",
    "    \n",
    "    output, state, output_list = unroll_lstm_cells(train_input_list, train_label_list)\n",
    "    \n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(output_list, 0), classifier_layer_weights, classifier_layer_biases)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf.concat(train_label_list, 0)))\n",
    "    \n",
    "    global_step = tf.Variable(0)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    \n",
    "    gradients_variables_tuple_list = clip_gradients_by_global_norm(optimizer)\n",
    "    \n",
    "    optimizer = optimizer.apply_gradients(gradients_variables_tuple_list, global_step = global_step)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1,num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    \n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "        sample_logits = tf.nn.xw_plus_b(sample_output, classifier_layer_weights, classifier_layer_biases)\n",
    "        sample_predictions = tf.nn.softmax(sample_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 3.342322\n",
      "Minibatch perplexity: 28.28\n",
      "================================================================================\n",
      "lebycvdh fvxpyymrqigqneahororbwqbfuwjpanaegsrhootiwhatzipldrbuzmzggteeanqhaitnya\n",
      "dcducfqdchlfzkodnyqkuozfgivirfhrtqi yexks paqxpqjb eeczp xmvvprxrgmlkywvuqezztwn\n",
      "dxezaoilrpyxzdtmzdnkpxqrrthyxoneivjetqjzkkvfmercrklofuwi pedbfv fcckxiztqdcxktjf\n",
      "lwmfzqpotolutxulawxmtvudkjzysrstucgttktuxkhzvxjscytspikzjhkgyilkcthzmtz y  vctss\n",
      "mktrdfxxsjjfsjks gyolurdijbhbuottyxjwqpwnyupuvvxxsghdttagujhwvqjksemnbdwzygmbyyh\n",
      "================================================================================\n",
      "Loss at step 100: 2.761147\n",
      "Minibatch perplexity: 15.82\n",
      "Loss at step 200: 2.667452\n",
      "Minibatch perplexity: 14.40\n",
      "Loss at step 300: 2.404439\n",
      "Minibatch perplexity: 11.07\n",
      "Loss at step 400: 2.390692\n",
      "Minibatch perplexity: 10.92\n",
      "Loss at step 500: 2.388447\n",
      "Minibatch perplexity: 10.90\n",
      "Loss at step 600: 2.447060\n",
      "Minibatch perplexity: 11.55\n",
      "Loss at step 700: 2.399165\n",
      "Minibatch perplexity: 11.01\n",
      "Loss at step 800: 2.359375\n",
      "Minibatch perplexity: 10.58\n",
      "Loss at step 900: 2.475052\n",
      "Minibatch perplexity: 11.88\n",
      "Loss at step 1000: 2.357532\n",
      "Minibatch perplexity: 10.56\n",
      "================================================================================\n",
      "elteictorasibs sol ham tion ionde he speal ied oyts byutesalaigatotedseatlm lhes\n",
      "n yenunicole soncislseiafker tpu tho bomruifo mer forpe eyat one wil mite ce try\n",
      "de ing uic the sewed wito cople of eushe dlam aps of wiin hits eoqheron the tor \n",
      "qalaiofiynt urturelijecie tian tureapowoure cof concy el al tems bele hethemiljm\n",
      "wor fin n si zero wovtr to balye ata ayils zeugdsme foen sifutid ghe caubcaanllh\n",
      "================================================================================\n",
      "Loss at step 1100: 2.299529\n",
      "Minibatch perplexity: 9.97\n",
      "Loss at step 1200: 2.230982\n",
      "Minibatch perplexity: 9.31\n",
      "Loss at step 1300: 2.307320\n",
      "Minibatch perplexity: 10.05\n",
      "Loss at step 1400: 2.464369\n",
      "Minibatch perplexity: 11.76\n",
      "Loss at step 1500: 2.193479\n",
      "Minibatch perplexity: 8.97\n",
      "Loss at step 1600: 2.466231\n",
      "Minibatch perplexity: 11.78\n",
      "Loss at step 1700: 2.376688\n",
      "Minibatch perplexity: 10.77\n",
      "Loss at step 1800: 2.187938\n",
      "Minibatch perplexity: 8.92\n",
      "Loss at step 1900: 2.042383\n",
      "Minibatch perplexity: 7.71\n",
      "Loss at step 2000: 2.028411\n",
      "Minibatch perplexity: 7.60\n",
      "================================================================================\n",
      "preesed mad we l re tringd reqer ceveuth that it thiidp if cka in one the sus wu\n",
      "w cuybeass laruian eid fich wigte the frorelst it a ags one ofe the aro tiegtlob\n",
      "y fins in lor peremtuatheleye sher on chotem cot ljat mecimet in oniafe nith mar\n",
      "s eipne tneigelav dims to for hir nist beserd infh anc two hol fin taur ok ounb \n",
      "y thon af the zero beash dedtil waseh the mear at in livetons sthode fettiesooa \n",
      "================================================================================\n",
      "Loss at step 2100: 2.150825\n",
      "Minibatch perplexity: 8.59\n",
      "Loss at step 2200: 2.139509\n",
      "Minibatch perplexity: 8.50\n",
      "Loss at step 2300: 2.206349\n",
      "Minibatch perplexity: 9.08\n",
      "Loss at step 2400: 2.121328\n",
      "Minibatch perplexity: 8.34\n",
      "Loss at step 2500: 2.288976\n",
      "Minibatch perplexity: 9.86\n",
      "Loss at step 2600: 2.054281\n",
      "Minibatch perplexity: 7.80\n",
      "Loss at step 2700: 1.966164\n",
      "Minibatch perplexity: 7.14\n",
      "Loss at step 2800: 2.083227\n",
      "Minibatch perplexity: 8.03\n",
      "Loss at step 2900: 2.068970\n",
      "Minibatch perplexity: 7.92\n",
      "Loss at step 3000: 2.174494\n",
      "Minibatch perplexity: 8.80\n",
      "================================================================================\n",
      "b al antion kbtconland in a ta gwie gegptten kor memaranisalons jortre onpl fotu\n",
      "ze nas poldave to baring natinm is tho one govhilativeangarene al thes bas  in m\n",
      "xe ewcur a drata carest asd ams aicef one  in dose of the ene yal cornco of ras \n",
      "trour s fie sivey insder crc c of tto eas pavtin tive on  an dore ges zere one c\n",
      "yontct a zecan of stact s cochy wov lues fin tive sid a mergem severandm of pons\n",
      "================================================================================\n",
      "Loss at step 3100: 2.150722\n",
      "Minibatch perplexity: 8.59\n",
      "Loss at step 3200: 2.213957\n",
      "Minibatch perplexity: 9.15\n",
      "Loss at step 3300: 2.145410\n",
      "Minibatch perplexity: 8.55\n",
      "Loss at step 3400: 2.138071\n",
      "Minibatch perplexity: 8.48\n",
      "Loss at step 3500: 2.155500\n",
      "Minibatch perplexity: 8.63\n",
      "Loss at step 3600: 2.190648\n",
      "Minibatch perplexity: 8.94\n",
      "Loss at step 3700: 2.162998\n",
      "Minibatch perplexity: 8.70\n",
      "Loss at step 3800: 2.072012\n",
      "Minibatch perplexity: 7.94\n",
      "Loss at step 3900: 2.140136\n",
      "Minibatch perplexity: 8.50\n",
      "Loss at step 4000: 1.998618\n",
      "Minibatch perplexity: 7.38\n",
      "================================================================================\n",
      "jrem the nub is beb marbuh the the heanise belsrel heph imptrs arteer sevengn la\n",
      "vet and dam pelp gory s cimrenthaver dom net collidak alsewaces absacharita catt\n",
      "inchh hirarev of the ras dashorish tom to llont and warrish or tre chewalisipith\n",
      "pety wery is one vermiem carferedow fer cite einstes dis mate rurorys stal witkr\n",
      "b cord isving for asopre nith onep ler cals anderesohd runmmlens user coled mogd\n",
      "================================================================================\n",
      "Loss at step 4100: 2.107615\n",
      "Minibatch perplexity: 8.23\n",
      "Loss at step 4200: 2.154980\n",
      "Minibatch perplexity: 8.63\n",
      "Loss at step 4300: 2.170097\n",
      "Minibatch perplexity: 8.76\n",
      "Loss at step 4400: 2.107916\n",
      "Minibatch perplexity: 8.23\n",
      "Loss at step 4500: 2.067126\n",
      "Minibatch perplexity: 7.90\n",
      "Loss at step 4600: 2.052521\n",
      "Minibatch perplexity: 7.79\n",
      "Loss at step 4700: 2.159912\n",
      "Minibatch perplexity: 8.67\n",
      "Loss at step 4800: 2.102229\n",
      "Minibatch perplexity: 8.18\n",
      "Loss at step 4900: 2.051566\n",
      "Minibatch perplexity: 7.78\n",
      "Loss at step 5000: 2.078662\n",
      "Minibatch perplexity: 7.99\n",
      "================================================================================\n",
      "fol and a howicityouwfo aprter zero eige hart xlt waled and masouw bel chintsrip\n",
      "q zeat the romidiot aur bacedins mpe wily the six adurly of areatio forinuatre i\n",
      "fress lcus hitency eng teltisut chot andiass of wotinainyawlest linned parp or n\n",
      "lein zero cons woy bisrd whil sels  ixftal veve the by to ples etpth mads mounen\n",
      " r f anemed of tha sorg and andaby one oneelam merhh and reatins roodor micliche\n",
      "================================================================================\n",
      "Loss at step 5100: 2.198019\n",
      "Minibatch perplexity: 9.01\n",
      "Loss at step 5200: 2.008530\n",
      "Minibatch perplexity: 7.45\n",
      "Loss at step 5300: 2.119312\n",
      "Minibatch perplexity: 8.33\n",
      "Loss at step 5400: 2.219490\n",
      "Minibatch perplexity: 9.20\n",
      "Loss at step 5500: 2.121529\n",
      "Minibatch perplexity: 8.34\n",
      "Loss at step 5600: 2.090752\n",
      "Minibatch perplexity: 8.09\n",
      "Loss at step 5700: 2.125761\n",
      "Minibatch perplexity: 8.38\n",
      "Loss at step 5800: 2.108309\n",
      "Minibatch perplexity: 8.23\n",
      "Loss at step 5900: 2.206480\n",
      "Minibatch perplexity: 9.08\n",
      "Loss at step 6000: 2.104292\n",
      "Minibatch perplexity: 8.20\n",
      "================================================================================\n",
      "vei aster soustiac litterninginbaby sivetunsted thact suffpaaghers inscish is gr\n",
      "ath stbing the fuser thatitiz tars par zer thic teraoenmuv pro ins fusay one of \n",
      "s eoc inttorp spoler sooinpl looagicalbliwesoled hegying in ticmnes cisiofe heag\n",
      "ql bl thao f on oledfos the the pront fil filicplc to bestorit cirols chal pers \n",
      "ho is arrowinieyc pre in conhbers threere heficats co d odard nige one fise isis\n",
      "================================================================================\n",
      "Loss at step 6100: 2.198341\n",
      "Minibatch perplexity: 9.01\n",
      "Loss at step 6200: 2.127194\n",
      "Minibatch perplexity: 8.39\n",
      "Loss at step 6300: 2.240043\n",
      "Minibatch perplexity: 9.39\n",
      "Loss at step 6400: 2.053429\n",
      "Minibatch perplexity: 7.79\n",
      "Loss at step 6500: 2.106244\n",
      "Minibatch perplexity: 8.22\n",
      "Loss at step 6600: 2.257946\n",
      "Minibatch perplexity: 9.56\n",
      "Loss at step 6700: 2.298285\n",
      "Minibatch perplexity: 9.96\n",
      "Loss at step 6800: 2.055543\n",
      "Minibatch perplexity: 7.81\n",
      "Loss at step 6900: 2.010128\n",
      "Minibatch perplexity: 7.46\n",
      "Loss at step 7000: 2.128773\n",
      "Minibatch perplexity: 8.40\n",
      "================================================================================\n",
      "fo fiwe in the regir form tho the mum foolne uni nk moyes emoeanchres regkh two \n",
      "aced orf in are loon terrter mt perisharorsalytenter pan in wemiredones hirter w\n",
      " sf en to den dedamearf  tan itoionherealker naapl in tho zero fore partsaticts \n",
      "an the moanicedich in the cufoedteraling ara abdeting mrimige str stp in ors fol\n",
      "cul wualfed of the peo ther moamina preehtevonurlents pofd fican infere to rynas\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph_one_matmul) as session:\n",
    "    def build_feed_dict(batches):\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        return feed_dict\n",
    "    \n",
    "    def get_batches_perplexity(batches, predictions):\n",
    "        batches_labels = np.concatenate(list(batches)[1:])\n",
    "        perplexity = np.exp(logprob(predictions, batches_labels))\n",
    "        return float(perplexity)\n",
    "    \n",
    "    def generate_some_sentences_each_with_some_chars(sentences_num, chars_num):\n",
    "        sentences = list()\n",
    "        for _ in range(sentences_num):\n",
    "            sample_feed = sample(random_distribution())\n",
    "            sentence = characters(sample_feed)[0]\n",
    "            reset_sample_state.run()\n",
    "            \n",
    "            for _ in range(chars_num):\n",
    "                sample_predictions_train = sample_predictions.eval({sample_input: sample_feed})\n",
    "                sample_feed = sample(sample_predictions_train)\n",
    "                sentence += characters(sample_feed)[0]\n",
    "            sentences.append(sentence)\n",
    "            \n",
    "        return sentences\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    feed_dict = dict()\n",
    "    \n",
    "    for step in range(0, num_steps):\n",
    "        batches = train_batches.next()\n",
    "        \n",
    "        feed_dict = build_feed_dict(batches)\n",
    "        _, loss_train, train_prediction_train = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            print(\"Loss at step %d: %f\" % (step, loss_train))\n",
    "            batches_perplexity = get_batches_perplexity(batches, train_prediction_train)\n",
    "            print(\"Minibatch perplexity: %.2f\" % batches_perplexity)\n",
    "            \n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                print('=' * 80)\n",
    "                sentences = generate_some_sentences_each_with_some_chars(sentences_num = 5, chars_num = 79)\n",
    "                for sentence in sentences:\n",
    "                    print(''.join(sentence))\n",
    "                print('=' * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
